{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "import numpy as np\n",
    "from src.model.scalable_gmm import ScalableGMM, GMMConfig\n",
    "from src.utils.metrics import MetricsCollector, PerformanceMetrics\n",
    "from src.utils.validations import DataValidator\n",
    "from src.data.generator import DataGenerator\n",
    "from src.model.transformer import DataTransformer, TransformerConfig\n",
    "\n",
    "# Set up logging\n",
    "def setup_logging(log_level=logging.INFO):\n",
    "    logging.basicConfig(\n",
    "        level=log_level,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging()\n",
    "\n",
    "# Parse arguments\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description='VGM Processing Job')\n",
    "    parser.add_argument('--bucket_name', required=True, help='GCS bucket name')\n",
    "    parser.add_argument('--input_path', required=True, help='Input data path')\n",
    "    parser.add_argument('--output_path', required=True, help='Output path for transformed data')\n",
    "    parser.add_argument('--batch_size', type=int, default=1000, help='Batch size for processing')\n",
    "    parser.add_argument('--sample_size', type=int, default=100000, help='Sample size for GMM fitting')\n",
    "    parser.add_argument('--validation_threshold', type=float, default=0.05, help='Threshold for validation tests')\n",
    "    parser.add_argument('--validation_sample_size', type=int, default=10000, help='Sample size for validation')\n",
    "    return parser.parse_args()\n",
    "\n",
    "args = parse_arguments()\n",
    "\n",
    "# Create Spark session\n",
    "def create_spark_session():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"VGM_Processing\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "        .config(\"spark.shuffle.service.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"1000\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark = create_spark_session()\n",
    "\n",
    "# Load and prepare data\n",
    "def load_and_prepare_data():\n",
    "    logger.info(f\"Loading data from {args.input_path}\")\n",
    "    data = spark.read.parquet(args.input_path) \\\n",
    "        .select(\"Amount\") \\\n",
    "        .repartition(1000)\n",
    "    data.cache()\n",
    "    total_records = data.count()\n",
    "    logger.info(f\"Loaded {total_records:,} records\")\n",
    "    return data\n",
    "\n",
    "data = load_and_prepare_data()\n",
    "\n",
    "# Fit VGM model\n",
    "def fit_vgm_model():\n",
    "    gmm_config = GMMConfig(\n",
    "        n_components=10,\n",
    "        batch_size=args.batch_size,\n",
    "        eps=0.005\n",
    "    )\n",
    "    vgm = ScalableGMM(gmm_config)\n",
    "    vgm.fit(data)\n",
    "    return vgm\n",
    "\n",
    "vgm = fit_vgm_model()\n",
    "\n",
    "# Transform data\n",
    "def transform_data():\n",
    "    logger.info(\"Transforming full dataset\")\n",
    "    transformed_data = vgm.transform(data)\n",
    "    return transformed_data\n",
    "\n",
    "transformed_data = transform_data()\n",
    "\n",
    "# Validate transformation\n",
    "def validate_transformation(validator, original_data, transformed_data, inverse_transformed_data):\n",
    "    # Perform validation\n",
    "    accuracy = validate_transformation(\n",
    "        validator,\n",
    "        original_data,\n",
    "        None,\n",
    "        inverse_transformed_data,\n",
    "        logger\n",
    "    )\n",
    "    return accuracy\n",
    "\n",
    "# Sample data for validation\n",
    "sample_size = min(args.validation_sample_size, data.count())\n",
    "sample_original = data.limit(sample_size).toPandas()[\"Amount\"].values\n",
    "sample_transformed = transformed_data.limit(sample_size)\n",
    "sample_inverse = vgm.inverse_transform(sample_transformed).toPandas()[\"Amount\"].values\n",
    "\n",
    "# Validate transformation\n",
    "validator = DataValidator(threshold=args.validation_threshold)\n",
    "accuracy = validate_transformation(validator, sample_original, None, sample_inverse)\n",
    "logger.info(f\"Transformation accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Save transformed data\n",
    "logger.info(f\"Saving transformed data to {args.output_path}\")\n",
    "transformed_data.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(args.output_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
